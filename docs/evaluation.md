---
title: Evaluation
---
<p align="center">
  <a href="index.md"><b>Home</b></a> |
  <a href="dataset.md"><b>Dataset</b></a> |
  <a href="challenge.md"><b>Challenge</b></a> |
  <a href="download.md"><b>Download</b></a> |
  <a href="baselines.md"><b>Baselines</b></a> |
  <a href="faq.md"><b>FAQ</b></a>
</p>

<hr>


# Evaluation

## Task
Binary prediction: **consistent** vs **inconsistent** imageâ€“caption pair.

## Metrics (typical)
Pick and publish what your leaderboard uses:
- Accuracy
- Macro-F1 (or binary F1)
- AUROC (if confidence scores are required)

## Optional: explanations (not scored)
Submitters may optionally provide:
- image grounding (boxes/masks/heatmaps)
- highlighted caption spans

We may feature strong explainability outputs in workshop presentations.
